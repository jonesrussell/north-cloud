# North Cloud Environment Variables
# Copy this file to .env and update with your values

# ============================================
# Global Configuration
# ============================================
APP_DEBUG=true
APP_ENV=development

# Docker User Configuration (Development)
# Set these to match your host user to avoid permission issues
# On Linux/Mac, run: id -u && id -g to get your UID and GID
UID=1000
GID=1000

# ============================================
# Infrastructure Services
# ============================================

# Elasticsearch
ELASTICSEARCH_PORT=9200
ELASTICSEARCH_SECURITY_ENABLED=false
ELASTICSEARCH_URL=http://localhost:9200

# Redis
REDIS_PORT=6379
REDIS_PASSWORD=
# Production: host Redis (crawler, publisher, classifier use this when Redis is not in compose)
REDIS_HOST=host.docker.internal

# Nginx
NGINX_HTTP_PORT=80
NGINX_HTTPS_PORT=443

# ============================================
# Source Manager Service
# ============================================
SOURCE_MANAGER_PORT=8050
SOURCE_MANAGER_HOST=0.0.0.0
SOURCE_MANAGER_API_URL=http://localhost:8050
FRONTEND_PORT=3000

# Source Manager Database
POSTGRES_SOURCE_MANAGER_HOST=localhost
POSTGRES_SOURCE_MANAGER_PORT=5432
POSTGRES_SOURCE_MANAGER_USER=postgres
POSTGRES_SOURCE_MANAGER_PASSWORD=postgres
POSTGRES_SOURCE_MANAGER_DB=source_manager
POSTGRES_SOURCE_MANAGER_SSLMODE=disable

# ============================================
# Crawler Service
# ============================================
CRAWLER_PORT=8060
CRAWLER_HOST=0.0.0.0
CRAWLER_SOURCES_API_URL=http://source-manager:8050/api/v1/sources
CRAWLER_SAVE_DISCOVERED_LINKS=true
# Colly: respect robots.txt (default true); set false to ignore
CRAWLER_RESPECT_ROBOTS_TXT=true
# Colly: rotate user agent per request; set true for UA rotation
CRAWLER_USE_RANDOM_USER_AGENT=false
# Colly: fixed user agent when CRAWLER_USE_RANDOM_USER_AGENT=false
CRAWLER_USER_AGENT=Mozilla/5.0 (compatible; NorthCloud/1.0; +https://northcloud.biz)

# Crawler Database
POSTGRES_CRAWLER_HOST=localhost
POSTGRES_CRAWLER_PORT=5433
POSTGRES_CRAWLER_USER=postgres
POSTGRES_CRAWLER_PASSWORD=postgres
POSTGRES_CRAWLER_DB=crawler
POSTGRES_CRAWLER_SSLMODE=disable

# Crawler Job Logs (Live Streaming + Archival)
JOB_LOGS_ENABLED=true
JOB_LOGS_BUFFER_SIZE=1000
JOB_LOGS_SSE_ENABLED=true
JOB_LOGS_ARCHIVE_ENABLED=true
JOB_LOGS_RETENTION_DAYS=30
JOB_LOGS_MIN_LEVEL=info
JOB_LOGS_MINIO_BUCKET=crawler-logs
JOB_LOGS_MILESTONE_INTERVAL=50
# Redis Streams for Job Logs (enables /api/v1/jobs/:id/logs/stream/v2 endpoint)
JOB_LOGS_REDIS_ENABLED=true
JOB_LOGS_REDIS_KEY_PREFIX=logs
JOB_LOGS_REDIS_TTL_SECONDS=86400

# ============================================
# Index Manager Service
# ============================================
POSTGRES_INDEX_MANAGER_HOST=localhost
POSTGRES_INDEX_MANAGER_PORT=5436
POSTGRES_INDEX_MANAGER_USER=postgres
POSTGRES_INDEX_MANAGER_PASSWORD=yourpassword
POSTGRES_INDEX_MANAGER_DB=index_manager

# ============================================
# Classifier Service
# ============================================
CLASSIFIER_PORT=8071
CLASSIFIER_HOST=0.0.0.0
CLASSIFIER_ENABLED=true
CLASSIFIER_CONCURRENCY=10
CLASSIFIER_BATCH_SIZE=100
CLASSIFIER_MIN_QUALITY_SCORE=50
CLASSIFIER_POLLING_INTERVAL=30s

# Coforge ML Classifier
COFORGE_ENABLED=true
COFORGE_ML_SERVICE_URL=http://coforge-ml:8078

# Entertainment ML Classifier
ENTERTAINMENT_ENABLED=false
ENTERTAINMENT_ML_SERVICE_URL=http://entertainment-ml:8079

# Classifier Database
POSTGRES_CLASSIFIER_HOST=localhost
POSTGRES_CLASSIFIER_PORT=5435
POSTGRES_CLASSIFIER_USER=postgres
POSTGRES_CLASSIFIER_PASSWORD=postgres
POSTGRES_CLASSIFIER_DB=classifier
POSTGRES_CLASSIFIER_SSLMODE=disable

# ============================================
# Search Service
# ============================================
SEARCH_PORT=8090
SEARCH_DEBUG=true
SEARCH_MAX_PAGE_SIZE=100
SEARCH_DEFAULT_PAGE_SIZE=20
SEARCH_LOG_LEVEL=info
SEARCH_LOG_FORMAT=json

# ============================================
# Auth Service
# ============================================
AUTH_SERVICE_PORT=8040
AUTH_SERVICE_HOST=0.0.0.0

# Auth Database
POSTGRES_AUTH_HOST=localhost
POSTGRES_AUTH_PORT=5432
POSTGRES_AUTH_USER=postgres
POSTGRES_AUTH_PASSWORD=changeme
POSTGRES_AUTH_DB=auth
POSTGRES_AUTH_SSLMODE=disable

# JWT Configuration
AUTH_JWT_SECRET=CpgfEP9nKSzLJTzfSJKW8ynwsOXrlQJ1h8ZQRbYSBJ4=
AUTH_JWT_EXPIRY=24h
AUTH_JWT_REFRESH_EXPIRY=168h

# Initial Admin User (for seed command)
AUTH_ADMIN_PASSWORD=changeme

# ============================================
# Publisher Service
# ============================================
PUBLISHER_PORT=8070

# Publisher Database
POSTGRES_PUBLISHER_HOST=localhost
POSTGRES_PUBLISHER_PORT=5437
POSTGRES_PUBLISHER_USER=postgres
POSTGRES_PUBLISHER_PASSWORD=postgres
POSTGRES_PUBLISHER_DB=publisher
POSTGRES_PUBLISHER_SSLMODE=disable

# Router Service Configuration
PUBLISHER_ROUTER_CHECK_INTERVAL=5m
PUBLISHER_ROUTER_BATCH_SIZE=100

# ============================================
# Pipeline Service
# ============================================
PIPELINE_PORT=8075
POSTGRES_PIPELINE_HOST=localhost
POSTGRES_PIPELINE_PORT=5438
POSTGRES_PIPELINE_USER=postgres
POSTGRES_PIPELINE_PASSWORD=postgres
POSTGRES_PIPELINE_DB=pipeline
POSTGRES_PIPELINE_SSLMODE=disable

# ============================================
# nc-http-proxy (HTTP Replay Proxy for Crawler)
# ============================================
PROXY_PORT=8055
PROXY_MODE=replay

# ============================================
# MinIO Object Storage (HTML Archiving)
# ============================================
MINIO_API_PORT=9000
MINIO_CONSOLE_PORT=9001
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=changeme123
HTML_RETENTION_DAYS=90

# Crawler MinIO Integration
CRAWLER_MINIO_ENABLED=true
CRAWLER_MINIO_ENDPOINT=minio:9000
CRAWLER_MINIO_ACCESS_KEY=minioadmin
CRAWLER_MINIO_SECRET_KEY=changeme123
CRAWLER_MINIO_BUCKET=html-archives
CRAWLER_MINIO_METADATA_BUCKET=crawler-metadata
CRAWLER_MINIO_USE_SSL=false

# ============================================
# Profiling Configuration (Development)
# ============================================
# Enable pprof profiling endpoints on all Go services
# When enabled, each service exposes profiling data at http://localhost:{PORT}/debug/pprof/
# Set to false in production or when profiling is not needed (zero overhead when disabled)
ENABLE_PROFILING=true

# Profiling Ports (Development Only)
# Each service runs pprof on a separate port for concurrent profiling
# Access profiles via: curl http://localhost:{PORT}/debug/pprof/heap -o heap.pprof
CRAWLER_PPROF_PORT=6060
SOURCE_MANAGER_PPROF_PORT=6061
CLASSIFIER_PPROF_PORT=6062
PUBLISHER_API_PPROF_PORT=6063
PUBLISHER_ROUTER_PPROF_PORT=6064
AUTH_PPROF_PORT=6065
SEARCH_PPROF_PORT=6066
PIPELINE_PPROF_PORT=6067

# Continuous Profiling (Optional - Pyroscope)
# Enable continuous profiling with Grafana Pyroscope for production-grade observability
# When enabled, profiles are continuously sent to Pyroscope server with <2% overhead
ENABLE_CONTINUOUS_PROFILING=false
PYROSCOPE_PORT=4040
PYROSCOPE_SERVER_URL=http://pyroscope:4040
PYROSCOPE_ENVIRONMENT=development
PYROSCOPE_LOG_LEVEL=debug

# ============================================
# Docker BuildKit Configuration
# ============================================
# Limit BuildKit parallelism to prevent RAM exhaustion during builds
# Set to 1 to build one service at a time (safest, uses less RAM)
# Set to 2-3 to build multiple services in parallel (faster, uses more RAM)
# Uncomment and set if experiencing high RAM usage during docker compose build
# BUILDKIT_MAX_PARALLELISM=1

# Alternative: Disable BuildKit entirely (uses legacy builder)
# DOCKER_BUILDKIT=0

# ============================================
# Database Backup Configuration
# ============================================
DB_BACKUP_DIR=./backups
DB_BACKUP_RETENTION=7

# Production Sync (development only - for pulling prod data to dev)
# PROD_SSH_HOST=user@production-server
# PROD_SSH_PORT=22
# PROD_DEPLOY_PATH=/opt/north-cloud

# ============================================
# Logging Infrastructure (Loki + Promtail + Grafana)
# ============================================

# Loki Configuration
LOKI_PORT=3100
LOKI_RETENTION_DAYS=7              # Log retention (7 for dev, 30 for prod)
LOKI_INGESTION_RATE_MB=5           # Max ingestion rate per stream
LOKI_INGESTION_BURST_MB=10         # Max burst size

# Grafana Alloy Configuration
# Next-generation telemetry collector (replaced Promtail which reached EOL March 2, 2026)
ALLOY_PORT=12345                   # Alloy UI/API port

# Grafana Configuration
GRAFANA_PORT=3000
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=changeme    # CHANGE IN PRODUCTION
GRAFANA_ANONYMOUS_ENABLED=false    # Allow anonymous viewing (true for dev, false for prod)
GRAFANA_ROOT_URL=http://localhost:3000  # Production: https://yourdomain.com/grafana
GRAFANA_LOG_MODE=console
GRAFANA_LOG_LEVEL=info
GRAFANA_INSTALL_PLUGINS=           # Optional plugins (comma-separated)
