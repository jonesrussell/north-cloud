# ============================================================
# YAML Anchors (DRY patterns)
# ============================================================

x-postgres-defaults: &postgres-defaults
  image: postgres:16-alpine
  restart: always
  deploy:
    resources:
      limits:
        cpus: "0.5"
        memory: 512M
  healthcheck:
    test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres}"]
    interval: 5s
    timeout: 5s
    retries: 5
  networks:
    - north-cloud-network

x-service-defaults: &service-defaults
  restart: always
  networks:
    - north-cloud-network

# ============================================================
# Services
# ============================================================

services:

  # ------------------------------------------------------------
  # PostgreSQL Instances
  # ------------------------------------------------------------

  postgres-source-manager:
    <<: *postgres-defaults
    environment:
      POSTGRES_USER: ${POSTGRES_SOURCE_MANAGER_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_SOURCE_MANAGER_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_SOURCE_MANAGER_DB:-source_manager}
    volumes:
      - postgres_source_manager_data:/var/lib/postgresql/data
      - ./source-manager/migrations:/migrations:ro
      - ./infrastructure/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro

  postgres-crawler:
    <<: *postgres-defaults
    ports:
      - "${POSTGRES_CRAWLER_PORT:-5433}:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_CRAWLER_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_CRAWLER_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_CRAWLER_DB:-crawler}
    volumes:
      - postgres_crawler_data:/var/lib/postgresql/data
      - ./crawler/migrations:/migrations:ro

  postgres-classifier:
    <<: *postgres-defaults
    environment:
      POSTGRES_USER: ${POSTGRES_CLASSIFIER_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_CLASSIFIER_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_CLASSIFIER_DB:-classifier}
    volumes:
      - postgres_classifier_data:/var/lib/postgresql/data
      - ./classifier/migrations:/migrations:ro

  postgres-index-manager:
    <<: *postgres-defaults
    environment:
      POSTGRES_USER: ${POSTGRES_INDEX_MANAGER_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_INDEX_MANAGER_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_INDEX_MANAGER_DB:-index_manager}
    volumes:
      - postgres_index_manager_data:/var/lib/postgresql/data
      - ./index-manager/migrations:/migrations:ro

  postgres-publisher:
    <<: *postgres-defaults
    environment:
      POSTGRES_USER: ${POSTGRES_PUBLISHER_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PUBLISHER_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_PUBLISHER_DB:-publisher}
    volumes:
      - postgres_publisher_data:/var/lib/postgresql/data
      - ./publisher/migrations:/migrations:ro

  postgres-pipeline:
    <<: *postgres-defaults
    environment:
      POSTGRES_USER: ${POSTGRES_PIPELINE_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PIPELINE_PASSWORD:-postgres}
      POSTGRES_DB: ${POSTGRES_PIPELINE_DB:-pipeline}
    volumes:
      - postgres_pipeline_data:/var/lib/postgresql/data
      - ./pipeline/migrations:/migrations:ro

  # ------------------------------------------------------------
  # Elasticsearch
  # ------------------------------------------------------------

  elasticsearch:
    <<: *service-defaults
    image: docker.elastic.co/elasticsearch/elasticsearch:9.2.2
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: 1G
    shm_size: 256mb
    environment:
      discovery.type: single-node
      xpack.security.enabled: ${ELASTICSEARCH_SECURITY_ENABLED:-false}
      ES_JAVA_OPTS: "-Xms${ELASTICSEARCH_MIN_HEAP:-512m} -Xmx${ELASTICSEARCH_MAX_HEAP:-512m}"
      bootstrap.memory_lock: "true"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - /mnt/volume_tor1_01_es:/usr/share/elasticsearch/data
      - ./infrastructure/elasticsearch/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health?wait_for_status=green&timeout=30s || curl -f http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=30s || exit 1"]
      interval: 10s
      timeout: 35s
      retries: 10
      start_period: 60s

  # ------------------------------------------------------------
  # Redis
  # ------------------------------------------------------------

  redis:
    <<: *service-defaults
    image: redis:7-alpine
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 256M
    ports:
      - "${REDIS_PORT:-6379}:6379"
    command: >
      sh -c "
      if [ -n \"$$REDIS_PASSWORD\" ]; then
        redis-server --appendonly yes --protected-mode no --requirepass \"$$REDIS_PASSWORD\"
      else
        redis-server --appendonly yes --protected-mode no
      fi
      "
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD:-}
    volumes:
      - redis_data:/data
    healthcheck:
      test: >
        sh -c "
        if [ -n \"$$REDIS_PASSWORD\" ]; then
          redis-cli -a \"$$REDIS_PASSWORD\" ping
        else
          redis-cli ping
        fi
        "
      interval: 10s
      timeout: 5s
      retries: 5

  # ------------------------------------------------------------
  # MinIO + Init
  # ------------------------------------------------------------

  minio:
    <<: *service-defaults
    image: quay.io/minio/minio:latest
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 256M
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-changeme123}
      MINIO_BROWSER: ${MINIO_BROWSER:-on}
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 15s

  minio-init:
    image: quay.io/minio/mc:latest
    restart: "no"
    networks:
      - north-cloud-network
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      echo 'Waiting for MinIO to be ready...';
      until mc alias set local http://minio:9000 ${MINIO_ROOT_USER:-minioadmin} ${MINIO_ROOT_PASSWORD:-changeme123} 2>/dev/null; do
        echo 'MinIO not ready yet, retrying in 2 seconds...';
        sleep 2;
      done;
      echo 'MinIO is ready, initializing buckets...';
      mc mb --ignore-existing local/html-archives;
      mc mb --ignore-existing local/crawler-metadata;
      mc mb --ignore-existing local/crawler-logs;
      mc ilm rule add local/html-archives --expire-days ${HTML_RETENTION_DAYS:-90} --prefix 'archived/' || true;
      mc ilm rule add local/crawler-logs --expire-days ${JOB_LOGS_RETENTION_DAYS:-30} || true;
      mc version enable local/html-archives;
      echo 'MinIO buckets initialized successfully';
      exit 0;
      "

  # ------------------------------------------------------------
  # Application Services
  # ------------------------------------------------------------

  search-service:
    <<: *service-defaults
    # No profiles here so prod (base+prod) always has search-service; dev (base+dev) overrides with profiles: [search]
    build:
      context: .
      dockerfile: ./search/Dockerfile
    image: docker.io/jonesrussell/search-service:latest
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 256M
    environment:
      APP_DEBUG: ${APP_DEBUG:-true}
      SEARCH_PORT: ${SEARCH_PORT:-8090}
      SEARCH_DEBUG: ${SEARCH_DEBUG:-true}
      ELASTICSEARCH_URL: ${ELASTICSEARCH_URL:-http://elasticsearch:9200}
      ELASTICSEARCH_USERNAME: ${ELASTICSEARCH_USERNAME:-}
      ELASTICSEARCH_PASSWORD: ${ELASTICSEARCH_PASSWORD:-}
      LOG_LEVEL: ${SEARCH_LOG_LEVEL:-info}
      LOG_FORMAT: ${SEARCH_LOG_FORMAT:-json}
      CORS_ORIGINS: ${CORS_ORIGINS:-*}
    depends_on:
      - elasticsearch
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8090/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  search-frontend:
    <<: *service-defaults
    # No profiles here so prod always has search-frontend; dev overrides with profiles: [search]
    build:
      context: ./search-frontend
      dockerfile: Dockerfile
    image: docker.io/jonesrussell/search-frontend:latest
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
    depends_on:
      - search-service

  auth:
    <<: *service-defaults
    build:
      context: .
      dockerfile: ./auth/Dockerfile
    image: docker.io/jonesrussell/auth:latest
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
    environment:
      AUTH_USERNAME: ${AUTH_USERNAME:-admin}
      AUTH_PASSWORD: ${AUTH_PASSWORD:-admin}
      AUTH_JWT_SECRET: ${AUTH_JWT_SECRET:-}
      AUTH_PORT: ${AUTH_PORT:-8040}
      APP_DEBUG: ${APP_DEBUG:-false}
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8040/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

  # ------------------------------------------------------------
  # Crime ML Classifier
  # ------------------------------------------------------------

  crime-ml:
    <<: *service-defaults
    build:
      context: ./ml-sidecars/crime-ml
      dockerfile: Dockerfile
    image: docker.io/jonesrussell/crime-ml:latest
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
    environment:
      MODEL_PATH: /app/models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8076/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

  # ------------------------------------------------------------
  # Mining ML Classifier
  # ------------------------------------------------------------

  mining-ml:
    <<: *service-defaults
    build:
      context: ./ml-sidecars/mining-ml
      dockerfile: Dockerfile
    image: docker.io/jonesrussell/mining-ml:latest
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
    environment:
      MODEL_PATH: /app/models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8077/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

  # ------------------------------------------------------------
  # Coforge ML Classifier
  # ------------------------------------------------------------

  coforge-ml:
    <<: *service-defaults
    build:
      context: ./ml-sidecars/coforge-ml
      dockerfile: Dockerfile
    image: docker.io/jonesrussell/coforge-ml:latest
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
    environment:
      MODEL_PATH: /app/models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8078/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

  # ------------------------------------------------------------
  # Entertainment ML Classifier
  # ------------------------------------------------------------

  entertainment-ml:
    <<: *service-defaults
    build:
      context: ./ml-sidecars/entertainment-ml
      dockerfile: Dockerfile
    image: docker.io/jonesrussell/entertainment-ml:latest
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8079/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # ------------------------------------------------------------
  # HTTP Replay Proxy (nc-http-proxy)
  # ------------------------------------------------------------

  nc-http-proxy:
    <<: *service-defaults
    build:
      context: ./nc-http-proxy
      dockerfile: Dockerfile
    image: docker.io/jonesrussell/nc-http-proxy:latest
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 128M
    ports:
      - "${PROXY_PORT:-8055}:8055"
    environment:
      PROXY_PORT: "8055"
      PROXY_MODE: ${PROXY_MODE:-replay}
      PROXY_FIXTURES_DIR: /app/fixtures
      PROXY_CACHE_DIR: /app/cache
      PROXY_CERT_FILE: /app/certs/proxy.crt
      PROXY_KEY_FILE: /app/certs/proxy.key
    volumes:
      - ./crawler/fixtures:/app/fixtures:ro
      - nc_http_proxy_cache:/app/cache
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8055/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s

  # ------------------------------------------------------------
  # Logging / Observability (profile: observability)
  # ------------------------------------------------------------

  loki:
    <<: *service-defaults
    profiles:
      - observability
    image: grafana/loki:2.9.10
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
    ports:
      - "${LOKI_PORT:-3100}:3100"
    command: -config.file=/etc/loki/config.yml
    volumes:
      - ./infrastructure/loki/loki-config.yml:/etc/loki/config.yml:ro
      - loki_data:/loki
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Grafana Alloy - Next-generation telemetry collector
  # Replaced Promtail (which reached EOL March 2, 2026)
  alloy:
    <<: *service-defaults
    profiles:
      - observability
    image: grafana/alloy:latest
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: 256M
    ports:
      - "${ALLOY_PORT:-12345}:12345"  # Alloy UI/API port
    command:
      - run
      - --server.http.listen-addr=0.0.0.0:12345
      - --storage.path=/var/lib/alloy/data
      - /etc/alloy/config.alloy
    volumes:
      - ./infrastructure/alloy/config.alloy:/etc/alloy/config.alloy:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - alloy_data:/var/lib/alloy
    depends_on:
      loki:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "/bin/alloy", "tools", "check-config", "/etc/alloy/config.alloy"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  grafana:
    <<: *service-defaults
    profiles:
      - observability
    image: grafana/grafana:10.4.8
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: 512M
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    environment:
      GF_AUTH_ANONYMOUS_ENABLED: ${GRAFANA_ANONYMOUS_ENABLED:-false}
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-changeme}
      GF_SERVER_ROOT_URL: ${GRAFANA_ROOT_URL:-http://localhost:3000}
      GF_SERVER_SERVE_FROM_SUB_PATH: "true"
      GF_LOG_MODE: ${GRAFANA_LOG_MODE:-console}
      GF_LOG_LEVEL: ${GRAFANA_LOG_LEVEL:-info}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./infrastructure/grafana/provisioning:/etc/grafana/provisioning:ro
    depends_on:
      loki:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

# ============================================================
# Volumes & Networks
# ============================================================

volumes:
  postgres_source_manager_data:
  postgres_crawler_data:
  postgres_classifier_data:
  postgres_index_manager_data:
  postgres_publisher_data:
  postgres_pipeline_data:
  redis_data:
  minio_data:
  loki_data:
  alloy_data:
  grafana_data:
  nc_http_proxy_cache:

networks:
  north-cloud-network:
    driver: bridge
