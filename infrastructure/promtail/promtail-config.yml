# Promtail Configuration
# Collects logs from Docker containers and forwards to Loki

server:
  http_listen_port: 9080
  grpc_listen_port: 0
  log_level: info

positions:
  filename: /tmp/positions/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push
    timeout: 10s
    backoff_config:
      min_period: 500ms
      max_period: 5m
      max_retries: 10
    batchwait: 1s
    batchsize: 102400  # 100KB

scrape_configs:
  # Docker container logs via Docker API
  - job_name: docker
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
        filters:
          - name: label
            values: ["com.docker.compose.project=north-cloud"]

    relabel_configs:
      # Extract container name (remove leading slash and project prefix)
      - source_labels: ['__meta_docker_container_name']
        regex: '/north-cloud-(.+?)(?:-\d+)?'
        target_label: 'service'
        replacement: '${1}'

      # Extract service name from Docker Compose label (more reliable)
      - source_labels: ['__meta_docker_container_label_com_docker_compose_service']
        target_label: 'service'

      # Extract project name from Docker Compose label
      - source_labels: ['__meta_docker_container_label_com_docker_compose_project']
        target_label: 'project'

      # Ensure project label is always set (fallback if Docker label missing)
      # This guarantees consistent labeling even if Docker labels are inconsistent
      - target_label: 'project'
        replacement: 'north-cloud'
        action: replace
        source_labels: []

      # Extract container ID (short version, first 12 chars)
      - source_labels: ['__meta_docker_container_id']
        regex: '^(.{12}).*'
        target_label: 'container_id'

      # Add job label
      - target_label: 'job'
        replacement: 'docker'

    pipeline_stages:
      # Parse JSON logs from Docker (Docker wraps logs in JSON with 'log', 'stream', 'time' fields)
      - json:
          expressions:
            log: log
            stream: stream
            time: time

      # Parse the actual log line as JSON (all services now use JSON format)
      # Expected: All North Cloud services output structured JSON logs
      # If parsing fails, structured fields (level, service, etc.) won't be available
      # This is logged as a warning by Promtail but doesn't drop the log line
      - json:
          expressions:
            level: level
            logger: logger
            caller: caller
            msg: msg
            ts: ts
            log_service: service  # Extract as log_service to avoid overwriting Docker service label
            method: method
            error: error
            duration: duration
            status_code: status_code
          source: log
          # Note: drop_malformed would drop non-JSON logs entirely
          # We keep it false so logs are still ingested, just without structured fields
          # Monitor Promtail logs for JSON parsing failures to identify misconfigured services

      # Normalize level field: ensure it exists (even if empty) for reliable matching
      # This allows match stages to work correctly whether level was extracted or not
      - template:
          source: level
          template: '{{ if .level }}{{ .level }}{{ else }}{{ end }}'

      # Extract level as a label (from JSON parsing if successful)
      # After normalization above, level will always exist (empty string if not extracted)
      - labels:
          level:

      # Fallback: Multi-stage level extraction if JSON parsing failed
      # Match stage only processes logs where level label is empty (JSON parsing failed)
      # This prevents unnecessary regex/logfmt processing when JSON already succeeded
      
      # 1. Try logfmt format (common in Go/Rust services: level=info msg="...")
      - match:
          selector: '{level=""}'
          stages:
            - logfmt:
                mapping:
                  level: level
            - labels:
                level:

      # 2. If logfmt also failed, try generic regex (last resort)
      # Matches: "level":"info", level="info", [INFO], INFO:, level=debug, etc.
      # More permissive pattern to catch various log format variations
      - match:
          selector: '{level=""}'
          stages:
            - regex:
                expression: '(?i)(?P<level>debug|info|warn|error|fatal)'
                source: log
            - labels:
                level:

      # If JSON parsing succeeded, use the parsed timestamp (more accurate than Docker's time)
      - timestamp:
          source: ts
          format: RFC3339Nano
          fallback_formats:
            - RFC3339
            - "2006-01-02T15:04:05.000Z07:00"
            - "2006-01-02T15:04:05Z07:00"
          on_failure: drop

      # Fallback to Docker's timestamp if JSON timestamp parsing failed
      - timestamp:
          source: time
          format: RFC3339Nano
          fallback_formats:
            - RFC3339
            - "2006-01-02T15:04:05.000Z07:00"
            - "2006-01-02T15:04:05Z07:00"
          on_failure: drop

      # Add stream label (stdout/stderr)
      - labels:
          stream:

      # Service label handling:
      # - Primary: Docker Compose service label (set in relabel_configs) - most reliable
      # - Fallback: log_service from JSON (only used if Docker label missing)
      # We extract log_service from JSON but don't automatically use it as a label
      # to avoid overwriting the reliable Docker-derived service label
      # If needed, you can query by log_service in LogQL: | json | log_service="..."

      # Output formatting - use msg if available (from JSON), otherwise use log (raw)
      - output:
          source: msg
          on_failure: log

  # Nginx access logs (if mounted)
  - job_name: nginx_access
    static_configs:
      - targets:
          - localhost
        labels:
          job: nginx
          log_type: access
          __path__: /var/log/nginx/access.log

    pipeline_stages:
      # Parse nginx access log format
      - regex:
          expression: '^(?P<remote_addr>[\w\.]+) - (?P<remote_user>[\w-]+) \[(?P<time_local>.*?)\] "(?P<method>\w+) (?P<request>.*?) (?P<protocol>HTTP/[\d\.]+)" (?P<status>\d+) (?P<body_bytes_sent>\d+) "(?P<http_referer>.*?)" "(?P<http_user_agent>.*?)"'

      - timestamp:
          source: time_local
          format: "02/Jan/2006:15:04:05 -0700"

      - labels:
          method:
          status:

      - static_labels:
          service: nginx
          level: info

  # Nginx error logs (if mounted)
  - job_name: nginx_error
    static_configs:
      - targets:
          - localhost
        labels:
          job: nginx
          log_type: error
          __path__: /var/log/nginx/error.log

    pipeline_stages:
      # Parse nginx error log format
      - regex:
          expression: '^(?P<timestamp>\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}) \[(?P<level>\w+)\] (?P<message>.*)'

      - timestamp:
          source: timestamp
          format: "2006/01/02 15:04:05"

      - labels:
          level:

      - static_labels:
          service: nginx

limits_config:
  readline_rate_enabled: false
  readline_rate: 10000
  readline_burst: 20000
