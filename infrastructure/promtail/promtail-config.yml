# Promtail Configuration
# Collects logs from Docker containers and forwards to Loki

server:
  http_listen_port: 9080
  grpc_listen_port: 0
  log_level: info

positions:
  filename: /tmp/positions/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push
    timeout: 10s
    backoff_config:
      min_period: 500ms
      max_period: 5m
      max_retries: 10
    batchwait: 1s
    batchsize: 102400  # 100KB

scrape_configs:
  # Docker container logs via Docker API
  - job_name: docker
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
        filters:
          - name: label
            values: ["com.docker.compose.project=north-cloud"]

    relabel_configs:
      # Extract container name (remove leading slash and project prefix)
      - source_labels: ['__meta_docker_container_name']
        regex: '/north-cloud-(.+?)(?:-\d+)?'
        target_label: 'service'
        replacement: '${1}'

      # Extract service name from Docker Compose label (preferred)
      - source_labels: ['__meta_docker_container_label_com_docker_compose_service']
        target_label: 'service'

      # Extract project name from Docker Compose label
      - source_labels: ['__meta_docker_container_label_com_docker_compose_project']
        target_label: 'project'

      # Fallback: only set project if missing (does NOT overwrite valid labels)
      - source_labels: ['project']
        regex: '^$'
        target_label: 'project'
        replacement: 'north-cloud'

      # Extract container ID (short version, first 12 chars)
      - source_labels: ['__meta_docker_container_id']
        regex: '^(.{12}).*'
        target_label: 'container_id'

      # Add job label
      - target_label: 'job'
        replacement: 'docker'

    pipeline_stages:
      # Parse JSON logs from Docker (Docker wraps logs in JSON with 'log', 'stream', 'time')
      - json:
          expressions:
            log: log
            stream: stream
            time: time

      # Parse the actual log line as JSON (North Cloud services output structured JSON)
      # NOTE: Promtail preserves the 'log' source field after parsing, so it's still available
      - json:
          expressions:
            level: level
            logger: logger
            caller: caller
            msg: msg
            ts: ts
            log_service: service   # Avoid overwriting Docker-derived service label
            method: method
            error: error
            duration: duration
            status_code: status_code
          source: log

      # Extract level as a label (for filtering/grouping in Grafana)
      - labels:
          level:

      # Fallback: generic regex for logs where JSON parsing failed
      # This works for both JSON and plain text logs that contain level keywords
      # More forgiving than logfmt, which fails when trying to parse JSON strings
      # Matches level keywords: debug, info, warn, error, fatal (case-insensitive)
      - match:
          selector: '{level=""}'
          stages:
            - regex:
                expression: '(?i)(?P<level>debug|info|warn|error|fatal)'
                source: log
            - labels:
                level:

      # Timestamp from JSON logs (preferred)
      - timestamp:
          source: ts
          format: RFC3339Nano
          fallback_formats:
            - RFC3339
            - "2006-01-02T15:04:05.000Z07:00"
            - "2006-01-02T15:04:05Z07:00"
          on_failure: drop

      # Fallback timestamp from Docker wrapper
      - timestamp:
          source: time
          format: RFC3339Nano
          fallback_formats:
            - RFC3339
            - "2006-01-02T15:04:05.000Z07:00"
            - "2006-01-02T15:04:05Z07:00"
          on_failure: drop

      # Add stream label (stdout/stderr)
      - labels:
          stream:

      # KEY FIX: Preserve JSON structure for Grafana sorting
      # Output the original 'log' field (service's JSON log) instead of just 'msg'
      # 
      # Why this works:
      # - The 'log' field contains the service's original JSON log line
      # - Promtail preserves the source field after JSON parsing, so 'log' still contains the JSON
      # - Grafana can re-parse this JSON with `| json` to extract fields for sorting
      # - Labels (like 'level') are still available for filtering/grouping
      # - Fields (level, method, status_code, etc.) are now available for sorting in the log table
      #
      # Fallback: If 'log' is missing (non-JSON logs), output 'msg' as last resort
      - output:
          source: log
          on_failure: msg

  # Nginx access logs (if mounted)
  - job_name: nginx_access
    static_configs:
      - targets:
          - localhost
        labels:
          job: nginx
          log_type: access
          __path__: /var/log/nginx/access.log

    pipeline_stages:
      - regex:
          expression: '^(?P<remote_addr>[\w\.]+) - (?P<remote_user>[\w-]+) \[(?P<time_local>.*?)\] "(?P<method>\w+) (?P<request>.*?) (?P<protocol>HTTP/[\d\.]+)" (?P<status>\d+) (?P<body_bytes_sent>\d+) "(?P<http_referer>.*?)" "(?P<http_user_agent>.*?)"'

      - timestamp:
          source: time_local
          format: "02/Jan/2006:15:04:05 -0700"

      - labels:
          method:
          status:

      - static_labels:
          service: nginx
          level: info

  # Nginx error logs (if mounted)
  - job_name: nginx_error
    static_configs:
      - targets:
          - localhost
        labels:
          job: nginx
          log_type: error
          __path__: /var/log/nginx/error.log

    pipeline_stages:
      - regex:
          expression: '^(?P<timestamp>\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}) \[(?P<level>\w+)\] (?P<message>.*)'

      - timestamp:
          source: timestamp
          format: "2006/01/02 15:04:05"

      - labels:
          level:

      - static_labels:
          service: nginx

limits_config:
  readline_rate_enabled: false
  readline_rate: 10000
  readline_burst: 20000
